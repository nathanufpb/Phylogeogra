# For each future climate projection, Worldclim V.2.1 provide a single raster file containing 19 bioclimatic bands:
bioclim_future <- terra::rast(paste0("Rasters/Global Predictors (FutureProjections)/", GlobalProjections[i]))
names(bioclim_future)
# Get SSP scenario and year:
folder_name1 <- gsub(".*.ssp", "", GlobalProjections[i])
folder_name1 <- gsub(".tif", "", folder_name1)
# Get the general circulation model (GCM):
folder_name2 <- gsub("_ssp.*", "", GlobalProjections[i])
folder_name2 <- gsub("wc2.1_10m_bioc_", "", folder_name2)
# Define the folder name:
mydir <- paste0("ssp", folder_name1, "_", folder_name2)
# Create the directory using the projection name:
dir.create(paste0("Projections/", mydir), showWarnings = FALSE)
dir.create(paste0("ProjectionsLowRes/", mydir), showWarnings = FALSE)
# Extract each band and export:
for(b in 1:length(var_names)){
# Load one raster band from the global projection i:
SingleBand <- terra::subset(x = bioclim_future, subset = b)
# Crop the raster band:
SingleBand <- raster::crop(x = SingleBand, y = Background_shp)
# Rename the raster layer:
names(SingleBand) <- paste0("wc2.1_10m_bio_", b) # change the spatial resolution if needed
# Write the raster:
terra::writeRaster(SingleBand, overwrite = TRUE,
filename = paste0("Projections/", mydir, "/", names(SingleBand), ".tif"))
# Reduce raster resolution to speed up computations:
SingleLayer <- terra::aggregate(x = SingleBand, fact = 2, fun = mean, overwrite = TRUE,
filename = paste0("ProjectionsLowRes/", mydir, "/", names(SingleBand), ".tif"))
}
}
# STEP 1.1: PREPARE RASTER FILES TO BE USED AS PREDICTORS AND FOR FUTURE PROJECTIONS (CMIP6 WORLDCLIM v2.1)
########################################################################################################################
# STEP 1.1: PREPARE RASTER FILES TO BE USED AS PREDICTORS AND FOR FUTURE PROJECTIONS (CMIP6 WORLDCLIM v2.1)
rm(list = ls()); gc()
# Load the shapefile on WWF Biogeographical realms:
WWF_realms <- rgdal::readOGR(dsn = "Shapefiles", layer = 'wwf_realm') # change directory as needed
# Select the Neotropical realm:
Background_shp <- WWF_realms[WWF_realms@data$wwf_realm == "Neotropic",]
rm(WWF_realms)
# Crop the extent of current predictor layers and export files:
path_current_predictors <- "Rasters/Global Predictors (CurrentTime)/"
bioclim_current <- terra::rast(list.files(path = path_current_predictors, pattern = '.tif', full.names = T))
# STEP 1.1: PREPARE RASTER FILES TO BE USED AS PREDICTORS AND FOR FUTURE PROJECTIONS (CMIP6 WORLDCLIM v2.1)
########################################################################################################################
# STEP 1.1: PREPARE RASTER FILES TO BE USED AS PREDICTORS AND FOR FUTURE PROJECTIONS (CMIP6 WORLDCLIM v2.1)
rm(list = ls()); gc()
# Load the shapefile on WWF Biogeographical realms:
WWF_realms <- rgdal::readOGR(dsn = "Shapefiles", layer = 'wwf_realm') # change directory as needed
# Select the Neotropical realm:
Background_shp <- WWF_realms[WWF_realms@data$wwf_realm == "Neotropic",]
rm(WWF_realms)
# Crop the extent of current predictor layers and export files:
path_current_predictors <- "Rasters/Global Predictors (CurrentTime)/"
bioclim_current <- terra::rast(list.files(path = path_current_predictors, pattern = '.tif', full.names = T))
var_names <- names(bioclim_current)
# Before starting, define the working directory and prepare R-packages needed:
# Please, inform where on your computer have you saved the folder 'Praticas'.
setwd("/home/nathan/Documents/Doutorado_diversidade_genética/Pseudosinella/Pseudosinella/Modelagem")
# Load all packages needed for the analysis:
load("RData/PackagesToLoad.RData")
lapply(all_packages, require, character.only = TRUE); rm(all_packages)
# Get a vector of non-installed packages:
new.packages <- all_packages[!(all_packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, repos = "http://cran.us.r-project.org")
# Load all packages needed for the analysis:
load("RData/PackagesToLoad.RData")
# STEP 1.1: PREPARE RASTER FILES TO BE USED AS PREDICTORS AND FOR FUTURE PROJECTIONS (CMIP6 WORLDCLIM v2.1)
########################################################################################################################
# STEP 1.1: PREPARE RASTER FILES TO BE USED AS PREDICTORS AND FOR FUTURE PROJECTIONS (CMIP6 WORLDCLIM v2.1)
rm(list = ls()); gc()
# Load the shapefile on WWF Biogeographical realms:
WWF_realms <- rgdal::readOGR(dsn = "Shapefiles", layer = 'wwf_realm') # change directory as needed
# Select the Neotropical realm:
Background_shp <- WWF_realms[WWF_realms@data$wwf_realm == "Neotropic",]
rm(WWF_realms)
# Crop the extent of current predictor layers and export files:
path_current_predictors <- "Rasters/Global Predictors (CurrentTime)/"
bioclim_current <- terra::rast(list.files(path = path_current_predictors, pattern = '.tif', full.names = T))
var_names <- names(bioclim_current)
# Extract each raster layer:
for(b in 1:length(var_names)){
# Select one raster layer:
SingleLayer <- terra::subset(x = bioclim_current, subset = b)
# Crop raster bands:
SingleLayer <- terra::crop(x = SingleLayer, y = Background_shp)
# Write the raster:
terra::writeRaster(SingleLayer,
filename = paste0("Predictors/", var_names[b], ".tif"),
overwrite = TRUE)
# Reduce raster resolution to speed up computations:
SingleLayer <- terra::aggregate(x = SingleLayer, fact = 2, method = 'bilinear', overwrite = TRUE,
filename = paste0("PredictorsLowRes/", var_names[b], ".tif"))
}
# Write the raster:
terra::writeRaster(SingleLayer,
filename = paste0("Predictors/", var_names[b], ".tif"),
overwrite = TRUE)
# Aumenta raster resolution to speed up computations:
SingleLayer <- terra::aggregate(x = SingleLayer, fact = 2, method = 'bilinear', overwrite = TRUE,
filename = paste0("PredictorsLowRes/", var_names[b], ".tif"))
# Extract each raster layer:
for(b in 1:length(var_names)){
# Select one raster layer:
SingleLayer <- terra::subset(x = bioclim_current, subset = b)
# Crop raster bands:
SingleLayer <- terra::crop(x = SingleLayer, y = Background_shp)
# Write the raster:
terra::writeRaster(SingleLayer,
filename = paste0("Predictors/", var_names[b], ".tif"),
overwrite = TRUE)
# Aumenta raster resolution to speed up computations:
SingleLayer <- terra::aggregate(x = SingleLayer, fact = 2, method = 'bilinear', overwrite = TRUE,
filename = paste0("PredictorsLowRes/", var_names[b], ".tif"))
}
# Get a list of all projections:
GlobalProjections <- dir("Rasters/Global Predictors (FutureProjections)/", all.files = F, recursive = T)
# Extract information on SSP, year, GCM, and spatial resolution of each global future projection:
Scenario_Year_GCM <- data.frame(
SSP = paste0("ssp", gsub("_.*", "", gsub(".tif", "", gsub(".*.ssp", "", GlobalProjections)))),
Year = gsub(".*_", "", gsub(".tif", "", gsub(".*.ssp", "", GlobalProjections))),
GCM = gsub(".*._bioc_", "", gsub(".ssp.*", "", GlobalProjections)),
Resolution = gsub("wc2.1_", "", gsub("_bioc_.*", "", gsub(".ssp.*", "", GlobalProjections))),
Filename = GlobalProjections)
# Filter global projections for a specific year or SSP, if necessary:
SelectedProjections <- Scenario_Year_GCM[Scenario_Year_GCM$SSP == "ssp245" & Scenario_Year_GCM$Year == "2081-2100",]
GlobalProjections <- SelectedProjections$Filename
# Unpack multi-band raster into individual files and save them:
for(i in 1:length(GlobalProjections)){
# For each future climate projection, Worldclim V.2.1 provide a single raster file containing 19 bioclimatic bands:
bioclim_future <- terra::rast(paste0("Rasters/Global Predictors (FutureProjections)/", GlobalProjections[i]))
names(bioclim_future)
# Get SSP scenario and year:
folder_name1 <- gsub(".*.ssp", "", GlobalProjections[i])
folder_name1 <- gsub(".tif", "", folder_name1)
# Get the general circulation model (GCM):
folder_name2 <- gsub("_ssp.*", "", GlobalProjections[i])
folder_name2 <- gsub("wc2.1_10m_bioc_", "", folder_name2)
# Define the folder name:
mydir <- paste0("ssp", folder_name1, "_", folder_name2)
# Create the directory using the projection name:
dir.create(paste0("Projections/", mydir), showWarnings = FALSE)
dir.create(paste0("ProjectionsLowRes/", mydir), showWarnings = FALSE)
# Extract each band and export:
for(b in 1:length(var_names)){
# Load one raster band from the global projection i:
SingleBand <- terra::subset(x = bioclim_future, subset = b)
# Crop the raster band:
SingleBand <- raster::crop(x = SingleBand, y = Background_shp)
# Rename the raster layer:
names(SingleBand) <- paste0("wc2.1_10m_bio_", b) # change the spatial resolution if needed
# Write the raster:
terra::writeRaster(SingleBand, overwrite = TRUE,
filename = paste0("Projections/", mydir, "/", names(SingleBand), ".tif"))
# Reduce raster resolution to speed up computations:
SingleLayer <- terra::aggregate(x = SingleBand, fact = 2, fun = mean, overwrite = TRUE,
filename = paste0("ProjectionsLowRes/", mydir, "/", names(SingleBand), ".tif"))
}
}
# STEP 2: PERFORM DATA CLEANING PROCEDURES IN THE SPECIES OCCURRENCE DATA
########################################################################################################################
# STEP 2: PERFORM DATA CLEANING PROCEDURES IN THE SPECIES OCCURRENCE DATA
rm(list = ls()); gc()
# Load the database:
CompiledDatabase <- data.table::fread(input = "Datasets/Pseudosinella_model.csv", encoding = "UTF-8")
head(CompiledDatabase)
# Extract the columns of interest:
CompiledDatabase <- CompiledDatabase[, c("Species", "Longitude", "Latitude", "ID")] # works for data.frame and data.table objects
CompiledDatabase$Longitude <- as.numeric(gsub(",", ".", CompiledDatabase$Longitude))
CompiledDatabase$Latitude <- as.numeric(gsub(",", ".", CompiledDatabase$Latitude))
# Remove rows without geographical coordinates:
CompiledDatabase <- CompiledDatabase[!is.na(CompiledDatabase$Latitude),]
CompiledDatabase <- CompiledDatabase[!is.na(CompiledDatabase$Longitude),]
str(CompiledDatabase)
#Check how many species:
nlevels(as.factor(CompiledDatabase$Species))
plot(study_area)
# Select the target species according to pre-defined study area.
# Load the shapefile of the WWF Ecoregions 2017 (Available at: https://ecoregions.appspot.com/):
study_area <- sf::st_read(dsn = "Shapefiles", layer = 'Neotropic')
# Let's check the structure of attributes within the loaded shapefile:
head(study_area)
plot(study_area)
# Create a object containing the coordinate reference system (crs) WGS84:
wgs84 <- "+proj=longlat +datum=WGS84 +no_defs"
# Create a spatialPoints object with the occurrence records:
pts <- sp::SpatialPoints(CompiledDatabase[, c("Longitude","Latitude")], proj4string = sp::CRS(wgs84))
# Check which occurrence records are within the desired ecoregion:
study_area <- sf::as_Spatial(study_area) # convert from sf to sp object
WithinStudyArea <- sp::over(x = pts, y = study_area)[, 1]
summary(WithinStudyArea)
rm(pts) # remove unnecessary objects
# Combine the 'WithinStudyArea' object with the CompiledDatabase:
CompiledDatabase <- cbind(CompiledDatabase, WithinStudyArea)
rm(WithinStudyArea)
# Identify which species have records within the study area:
CompiledDatabase$Species <- as.character(CompiledDatabase$Species) # convert to character to avoid matching problems
Target_spp <- unique(CompiledDatabase[!is.na(CompiledDatabase$WithinStudyArea),]$Species) # vector of species names
# Now, filter the CompiledDatabase to contain only the target species:
CompiledDatabase <- CompiledDatabase[Species %in% Target_spp,]
CompiledDatabase <- CompiledDatabase[ , c("Species","Longitude", "Latitude", "ID")] # keep only essential information
# Identify invalid Coordinates:
Output <- CoordinateCleaner::cc_val(CompiledDatabase, lon = "Longitude", lat = "Latitude", value = "flagged", verbose = T)
CompiledDatabase$InvalidOcc <- Output # add new column to register the dubious occurrences
CompiledDatabase <- CompiledDatabase[CompiledDatabase$InvalidOcc == TRUE, -c("InvalidOcc")] # remove dubious occurrences
# Identify Zero Coordinates (longitude = 0, latitude = 0):
Output <- CoordinateCleaner::cc_zero(CompiledDatabase, lon = "Longitude", lat = "Latitude", value = "flagged", verbose = T)
CompiledDatabase$ZeroCoord <- Output # add new column to register the zero coord occurrences
CompiledDatabase <- CompiledDatabase[CompiledDatabase$ZeroCoord==TRUE, -c("ZeroCoord")] # remove zero coord occurrences
# Identify records with Degree Conversion Error (degree sign treated as decimal delimiter):
CompiledDatabase$IDocc <- 1:nrow(CompiledDatabase)
Output <- CoordinateCleaner::cd_ddmm(CompiledDatabase, lon = "Longitude", lat = "Latitude", ds = "IDocc", value = "flagged", mat_size = 100)
CompiledDatabase$ConvError <- Output  # add new column to register the occurrences with conversion error
CompiledDatabase <- CompiledDatabase[CompiledDatabase$ConvError==TRUE, -c("ConvError")] # remove occurrences with conversion error
# Identify duplicates and remove them (different specimens, same species, same locality):
CompiledDatabase <- CoordinateCleaner::cc_dupl(CompiledDatabase, lon = "Longitude", lat = "Latitude", species = "Species", value = "clean")
# Identify occurrence records over the sea and flag them:
Output <- CoordinateCleaner::cc_sea(CompiledDatabase, lon = "Longitude", lat = "Latitude", value = "flagged", scale = 10)
CompiledDatabase$LandCoord <- Output # add new column to register the dubious occurrences
# Export the map and table of flagged records (manually check/curate if necessary):
flagged_records <- CompiledDatabase[CompiledDatabase$LandCoord=="FALSE",]
data.table::fwrite(x = flagged_records, file = "Datasets/FlaggedRecords.csv")
# Remove occurrence records falling in the ocean:
CompiledDatabase <- CompiledDatabase[CompiledDatabase$LandCoord==TRUE, ] # remove dubious occurrences
CompiledDatabase <- CompiledDatabase[, -c("LandCoord")] # remove the 'LandCoord' column
# Check how many occurrences per species:
N_occ_spp <- CompiledDatabase[,
.N, # count number of rows
by = Species] # provide countings per species
summary(N_occ_spp$N) # check the min and max number of occurrences per species
# Merge the column informing the number of records per species:
CompiledDatabase <- merge(x = CompiledDatabase,
y = N_occ_spp,
by = 'Species',
all.x = TRUE)
# Get the number of species remaining:
N_Spp <- levels(as.factor(CompiledDatabase$Species))
N_Spp
# For didactic purposes, we will use low resolution raster files in the subsequent analysis.
# Perform the spatial thinning of occurrence records (one occurrence per raster pixel or cell):
EnvRaster <- terra::rast("PredictorsLowRes//wc2.1_10m_bio_1.tif") # this was degradaded 2 times (20 minutes)
Occ <- list()
# For didactic purposes, we will use low resolution raster files in the subsequent analysis.
# Perform the spatial thinning of occurrence records (one occurrence per raster pixel or cell):
EnvRaster <- terra::rast("PredictorsLowRes//wc2.1_10m_bio_1.tif") # this was degradaded 2 times (20 minutes)
# Aumentar a resolução do raster
EnvRaster <- disaggregate(EnvRaster, fact = 2)
# Aumentar a resolução do raster
EnvRaster <- terra::aggregate(EnvRaster, fact = 0.5)
# For didactic purposes, we will use low resolution raster files in the subsequent analysis.
# Perform the spatial thinning of occurrence records (one occurrence per raster pixel or cell):
EnvRaster <- terra::rast("PredictorsLowRes//wc2.1_10m_bio_1.tif") # this was degradaded 2 times (20 minutes)
# Aumentar a resolução do raster
EnvRaster <- terra::aggregate(EnvRaster, fact = 0.5)
Occ <- list()
for(i in 1:length(N_Spp)){
try({
# Subset the occurrence records for the species 'i':
Occ_subset <- CompiledDatabase[CompiledDatabase$Species == N_Spp[i], c("Longitude", "Latitude")]
Occ_subset <- as.data.frame(Occ_subset)
names(Occ_subset) <- c("x", "y")
# Compute the spatial thinning of occurrence records:
Output <- flexsdm::occfilt_geo(data = Occ_subset,
x = "x",
y = "y",
method = c("cellsize", factor = "1"),
prj = crs(EnvRaster),
env_layer = EnvRaster)
Occ[[i]] <- data.frame(sp = N_Spp[i],
Output)
# Remove unnecessary objects before next iteration:
rm(Occ_subset, Output)
}, silent=T)
print(i)
}
# Bind all lists in a single data.table:
Occ <- rbindlist(Occ)
View(Occ)
# Load all packages needed for the analysis:
load("RData/PackagesToLoad.RData")
lapply(all_packages, require, character.only = TRUE); rm(all_packages)
# Get a vector of non-installed packages:
new.packages <- all_packages[!(all_packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, repos = "http://cran.us.r-project.org")
# STEP 1.1: PREPARE RASTER FILES TO BE USED AS PREDICTORS AND FOR FUTURE PROJECTIONS (CMIP6 WORLDCLIM v2.1)
########################################################################################################################
# STEP 1.1: PREPARE RASTER FILES TO BE USED AS PREDICTORS AND FOR FUTURE PROJECTIONS (CMIP6 WORLDCLIM v2.1)
rm(list = ls()); gc()
#definir área de estudo como sendo o estado de Minas Gerais a partir do pacote geobr
Background_shp <- geobr::read_state(code_state = "MG", year = 2018)
# Crop the extent of current predictor layers and export files:
path_current_predictors <- "/home/nathan/Documents/modelagem_ecologica/Rasters/Global Predictors (CurrentTime)/"
bioclim_current <- terra::rast(list.files(path = path_current_predictors, pattern = '.tif', full.names = T))
var_names <- names(bioclim_current)
# Extract each raster layer:
for(b in 1:length(var_names)){
# Select one raster layer:
SingleLayer <- terra::subset(x = bioclim_current, subset = b)
# Crop raster bands:
SingleLayer <- terra::crop(x = SingleLayer, y = Background_shp)
# Write the raster:
terra::writeRaster(SingleLayer,
filename = paste0("Predictors/", var_names[b], ".tif"),
overwrite = TRUE)
# Reduce raster resolution to speed up computations:
#SingleLayer <- terra::aggregate(x = SingleLayer, fact = 2, fun = mean, overwrite = TRUE,
#                             filename = paste0("PredictorsLowRes/", var_names[b], ".tif"))
}
# Get a list of all projections:
GlobalProjections <- dir("/home/nathan/Documents/modelagem_ecologica/Rasters/Global Predictors (FutureProjections)/", all.files = F, recursive = T)
# Extract information on SSP, year, GCM, and spatial resolution of each global future projection:
Scenario_Year_GCM <- data.frame(
SSP = paste0("ssp", gsub("_.*", "", gsub(".tif", "", gsub(".*.ssp", "", GlobalProjections)))),
Year = gsub(".*_", "", gsub(".tif", "", gsub(".*.ssp", "", GlobalProjections))),
GCM = gsub(".*._bioc_", "", gsub(".ssp.*", "", GlobalProjections)),
Resolution = gsub("wc2.1_", "", gsub("_bioc_.*", "", gsub(".ssp.*", "", GlobalProjections))),
Filename = GlobalProjections)
# Filter global projections for a specific year or SSP, if necessary:
SelectedProjections <- Scenario_Year_GCM[Scenario_Year_GCM$SSP == "ssp245" & Scenario_Year_GCM$Year == "2081-2100",]
GlobalProjections <- SelectedProjections$Filename
# Unpack multi-band raster into individual files and save them:
for(i in 1:length(GlobalProjections)){
# For each future climate projection, Worldclim V.2.1 provide a single raster file containing 19 bioclimatic bands:
bioclim_future <- terra::rast(paste0("/home/nathan/Documents/modelagem_ecologica/Rasters/Global Predictors (FutureProjections)/", GlobalProjections[i]))
names(bioclim_future)
# Get SSP scenario and year:
folder_name1 <- gsub(".*.ssp", "", GlobalProjections[i])
folder_name1 <- gsub(".tif", "", folder_name1)
# Get the general circulation model (GCM):
folder_name2 <- gsub("_ssp.*", "", GlobalProjections[i])
folder_name2 <- gsub("wc2.1_10m_bioc_", "", folder_name2)
# Define the folder name:
mydir <- paste0("ssp", folder_name1, "_", folder_name2)
# Create the directory using the projection name:
dir.create(paste0("Projections/", mydir), showWarnings = FALSE)
#dir.create(paste0("ProjectionsLowRes/", mydir), showWarnings = FALSE)
# Extract each band and export:
for(b in 1:length(var_names)){
# Load one raster band from the global projection i:
SingleBand <- bioclim_future[[b]]
# Crop the raster band:
SingleBand <- terra::crop(x = SingleBand, y = Background_shp)
# Rename the raster layer:
names(SingleBand) <- paste0("wc2.1_10m_bio_", b) # change the spatial resolution if needed
# Write the raster:
terra::writeRaster(SingleBand, overwrite = TRUE,
filename = paste0("Projections/", mydir, "/", names(SingleBand), ".tif"))
# Reduce raster resolution to speed up computations:
#SingleLayer <- terra::aggregate(x = SingleBand, fact = 2, fun = mean, overwrite = TRUE,
#                             filename = paste0("ProjectionsLowRes/", mydir, "/", names(SingleBand), ".tif"))
}
}
# STEP 2: PERFORM DATA CLEANING PROCEDURES IN THE SPECIES OCCURRENCE DATA
########################################################################################################################
# STEP 2: PERFORM DATA CLEANING PROCEDURES IN THE SPECIES OCCURRENCE DATA
rm(list = ls()); gc()
# Load the database:
CompiledDatabase <- data.table::fread(input = "Datasets/05_cleaned_database.csv", encoding = "UTF-8")
head(CompiledDatabase)
#mudar o nome das colunas 35 e 36 para Longitude e Latitude
names(CompiledDatabase)[35:36] <- c("Latitude", "Longitude")
#mudar o nome da coluna 45 para Species
names(CompiledDatabase)[45] <- "Species"
# Extract the columns of interest:
CompiledDatabase <- CompiledDatabase[, c("Species", "Longitude", "Latitude", "ID")] # works for data.frame and data.table objects
# Remove rows without geographical coordinates:
CompiledDatabase <- CompiledDatabase[!is.na(CompiledDatabase$Latitude),]
CompiledDatabase <- CompiledDatabase[!is.na(CompiledDatabase$Longitude),]
# Check how many species:
nlevels(as.factor(CompiledDatabase$Species))
#carregar a área de estudo como sendo o estado de Minas Gerais a partir do pacote geobr
study_area <- geobr::read_state(code_state = "MG", year = 2018)
# Let's check the structure of attributes within the loaded shapefile:
head(study_area)
# Create a object containing the coordinate reference system (crs) WGS84:
wgs84 <- "+proj=longlat +datum=WGS84 +no_defs"
pts <- sp::SpatialPoints(CompiledDatabase[, c("Longitude","Latitude")], proj4string = sp::CRS(wgs84))
study_area <- sf::st_transform(study_area, st_crs(pts))
# Check which occurrence records are within the desired ecoregion:
study_area <- sf::as_Spatial(study_area) # convert from sf to sp object
WithinStudyArea <- sp::over(x = pts, y = study_area)[, 1]
summary(WithinStudyArea)
rm(pts) # remove unnecessary objects
# Combine the 'WithinStudyArea' object with the CompiledDatabase:
CompiledDatabase <- cbind(CompiledDatabase, WithinStudyArea)
rm(WithinStudyArea)
# Identify which species have records within the study area:
CompiledDatabase$Species <- as.character(CompiledDatabase$Species) # convert to character to avoid matching problems
Target_spp <- unique(CompiledDatabase[!is.na(CompiledDatabase$WithinStudyArea),]$Species) # vector of species names
# Now, filter the CompiledDatabase to contain only the target species:
CompiledDatabase <- CompiledDatabase[Species %in% Target_spp,]
CompiledDatabase <- CompiledDatabase[ , c("Species","Longitude", "Latitude", "ID")] # keep only essential information
# Identify invalid Coordinates:
Output <- CoordinateCleaner::cc_val(CompiledDatabase, lon = "Longitude", lat = "Latitude", value = "flagged", verbose = T)
CompiledDatabase$InvalidOcc <- Output # add new column to register the dubious occurrences
CompiledDatabase <- CompiledDatabase[CompiledDatabase$InvalidOcc == TRUE, -c("InvalidOcc")] # remove dubious occurrences
# Identify Zero Coordinates (longitude = 0, latitude = 0):
Output <- CoordinateCleaner::cc_zero(CompiledDatabase, lon = "Longitude", lat = "Latitude", value = "flagged", verbose = T)
CompiledDatabase$ZeroCoord <- Output # add new column to register the zero coord occurrences
CompiledDatabase <- CompiledDatabase[CompiledDatabase$ZeroCoord==TRUE, -c("ZeroCoord")] # remove zero coord occurrences
# Identify records with Degree Conversion Error (degree sign treated as decimal delimiter):
CompiledDatabase$IDocc <- 1:nrow(CompiledDatabase)
Output <- CoordinateCleaner::cd_ddmm(CompiledDatabase, lon = "Longitude", lat = "Latitude", ds = "IDocc", value = "flagged", mat_size = 100)
CompiledDatabase$ConvError <- Output  # add new column to register the occurrences with conversion error
CompiledDatabase <- CompiledDatabase[CompiledDatabase$ConvError==TRUE, -c("ConvError")] # remove occurrences with conversion error
# Identify duplicates and remove them (different specimens, same species, same locality):
CompiledDatabase <- CoordinateCleaner::cc_dupl(CompiledDatabase, lon = "Longitude", lat = "Latitude", species = "Species", value = "clean")
# Corrigir latitude e longitude transpostas
check_pf$database_id <- 1:nrow(check_pf)
library(devtools)
devtools::install_github('gilles-guillot/Geneland', build_vignettes = TRUE)
devtools::install_github('gilles-guillot/Geneland', build_vignettes = TRUE)
library(tcltk)
library(tcltk, lib.loc = "/usr/local/lib/R/library")
library(tcltk)
detach("package:tcltk", unload = TRUE)
library(tcltk, lib.loc = "/usr/local/lib/R/library")
system("R CMD config --libs")
library(tcltk, lib.loc = "/usr/local/lib/R/library")
sudo apt-get update
library(tcltk, lib.loc = "/usr/local/lib/R/library")
capabilities("tcltk")
capabilities("tcltk")
clear
library(Geneland)
install.packages("ape")
library(ape)
install.packages("poppr")
library(poppr)
# Carregar o arquivo Nexus
nexus_file <- "/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/AMOVA/nuc_files/cmyc_phased_exon2.nex"
nexus_data <- read.nexus.data(nexus_file)
# Carregar o arquivo Nexus
nexus_file <- "/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/AMOVA/nuc_files/exon2/cmyc_phased_exon2.nex"
nexus_data <- read.nexus.data(nexus_file)
View(nexus_data)
# Transformar dados em formato de genótipos
genetic_data <- as.DNAbin(nexus_data)
# Criar a matriz de distância genética
genetic_dist <- dist.dna(genetic_data, model = "K80")
# Visualizar matriz de distância
print(genetic_dist)
View(nexus_data)
nexus_data[["Hap_15"]]
#definir diretório de trabalho
setwd("/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/AMOVA/nuc_files/exon2")
# Carregar o arquivo Nexus
nexus_file <- "/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/AMOVA/nuc_files/exon2/cmyc_phased_exon2.nex"
nexus_data <- read.nexus.data(nexus_file)
nexus_data <- read.nexus.data(nexus_file)
#definir diretório de trabalho
setwd("/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/AMOVA/nuc_files/exon2")
# Carregar o arquivo Nexus
nexus_file <- "/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/AMOVA/nuc_files/exon2/cmyc_phased_exon2.nex"
nexus_data <- read.nexus.data(nexus_file)
View(nexus_data)
?DNAbin
# Transformar dados em formato de genótipos
genetic_data <- as.?DNAbin(nexus_data)
# Transformar dados em formato de genótipos
genetic_data <- as.DNAbin(nexus_data)
View(genetic_data)
# Criar a matriz de distância genética
genetic_dist <- dist.dna(genetic_data, model = "K80")
View(genetic_data)
# Visualizar matriz de distância
print(genetic_dist)
# Carregar o arquivo Nexus
nexus_file <- "/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/AMOVA/nuc_files/exon2/cmyc_phased_exon2.nex"
nexus_data <- read.nexus(nexus_file)
nexus_data <- read.nexus.data(nexus_file)
View(nexus_data)
#definir diretório de trabalho
setwd("/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/AMOVA/nuc_files/exon2")
# Carregar o arquivo Nexus
nexus_file <- "/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/AMOVA/nuc_files/exon2/cmyc_phased_exon2.nex"
nexus_data <- read.nexus.data(nexus_file)
View(nexus_data)
# Transformar dados em formato de genótipos
genetic_data <- as.DNAbin(nexus_data)
View(genetic_data)
# Criar a matriz de distância genética
genetic_dist <- dist.dna(genetic_data, model = "K80")
# Visualizar matriz de distância
print(genetic_dist)
# Suponha que as 19 haplotipos estão divididos em 2 grupos
groups <- factor(c(rep("Group1", 10), rep("Group2", 9)))
# Configurar AMOVA
amova_result <- amova(genetic_dist ~ groups)
# Resumo dos resultados
summary(amova_result)
# Configurar AMOVA
amova_result <- amova(genetic_dist ~ groups)
#carregar arquivo fasta na pasta
fasta <- read.FASTA("cmyc-phased_exon2.fas")
View(fasta)
library(ape)
library(poppr)
#carregar arquivo fasta na pasta
alignment <- read.dna("cmyc-phased_exon2.fas", format = "fasta")
#definir diretório de trabalho
setwd("/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/AMOVA/nuc_files/exon2")
#carregar arquivo fasta na pasta
alignment <- read.dna("cmyc-phased_exon2.fas", format = "fasta")
#crie um vetor que associa cada sequência a um grupo se o ID da sequência começa com "AA", "TAP" ou "SM" defina como grupo norte se for "BC" , "BMD" ou  "FC" chame de grupo sul
groups <- ifelse(grepl("^AA|^TAP|^SM", names(alignment)), "Norte", "Sul")
library(pegas)
amova_result <- amova(dist.dna(alignment), ~ groups)
# Verificar se o número de sequências corresponde ao número de grupos
if (length(groups) != length(alignment)) {
stop("O número de grupos não corresponde ao número de sequências!")
}
names(alignment)
