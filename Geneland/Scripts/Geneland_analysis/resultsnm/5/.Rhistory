# STEP 2: PERFORM DATA CLEANING PROCEDURES IN THE SPECIES OCCURRENCE DATA
########################################################################################################################
# STEP 2: PERFORM DATA CLEANING PROCEDURES IN THE SPECIES OCCURRENCE DATA
rm(list = ls()); gc()
# Load the database:
CompiledDatabase <- data.table::fread(input = "Datasets/Pseudosinella_model.csv", encoding = "UTF-8")
head(CompiledDatabase)
# Extract the columns of interest:
CompiledDatabase <- CompiledDatabase[, c("Species", "Longitude", "Latitude", "ID")] # works for data.frame and data.table objects
CompiledDatabase$Longitude <- as.numeric(gsub(",", ".", CompiledDatabase$Longitude))
CompiledDatabase$Latitude <- as.numeric(gsub(",", ".", CompiledDatabase$Latitude))
# Remove rows without geographical coordinates:
CompiledDatabase <- CompiledDatabase[!is.na(CompiledDatabase$Latitude),]
CompiledDatabase <- CompiledDatabase[!is.na(CompiledDatabase$Longitude),]
str(CompiledDatabase)
#Check how many species:
nlevels(as.factor(CompiledDatabase$Species))
plot(study_area)
# Select the target species according to pre-defined study area.
# Load the shapefile of the WWF Ecoregions 2017 (Available at: https://ecoregions.appspot.com/):
study_area <- sf::st_read(dsn = "Shapefiles", layer = 'Neotropic')
# Let's check the structure of attributes within the loaded shapefile:
head(study_area)
plot(study_area)
# Create a object containing the coordinate reference system (crs) WGS84:
wgs84 <- "+proj=longlat +datum=WGS84 +no_defs"
# Create a spatialPoints object with the occurrence records:
pts <- sp::SpatialPoints(CompiledDatabase[, c("Longitude","Latitude")], proj4string = sp::CRS(wgs84))
# Check which occurrence records are within the desired ecoregion:
study_area <- sf::as_Spatial(study_area) # convert from sf to sp object
WithinStudyArea <- sp::over(x = pts, y = study_area)[, 1]
summary(WithinStudyArea)
rm(pts) # remove unnecessary objects
# Combine the 'WithinStudyArea' object with the CompiledDatabase:
CompiledDatabase <- cbind(CompiledDatabase, WithinStudyArea)
rm(WithinStudyArea)
# Identify which species have records within the study area:
CompiledDatabase$Species <- as.character(CompiledDatabase$Species) # convert to character to avoid matching problems
Target_spp <- unique(CompiledDatabase[!is.na(CompiledDatabase$WithinStudyArea),]$Species) # vector of species names
# Now, filter the CompiledDatabase to contain only the target species:
CompiledDatabase <- CompiledDatabase[Species %in% Target_spp,]
CompiledDatabase <- CompiledDatabase[ , c("Species","Longitude", "Latitude", "ID")] # keep only essential information
# Identify invalid Coordinates:
Output <- CoordinateCleaner::cc_val(CompiledDatabase, lon = "Longitude", lat = "Latitude", value = "flagged", verbose = T)
CompiledDatabase$InvalidOcc <- Output # add new column to register the dubious occurrences
CompiledDatabase <- CompiledDatabase[CompiledDatabase$InvalidOcc == TRUE, -c("InvalidOcc")] # remove dubious occurrences
# Identify Zero Coordinates (longitude = 0, latitude = 0):
Output <- CoordinateCleaner::cc_zero(CompiledDatabase, lon = "Longitude", lat = "Latitude", value = "flagged", verbose = T)
CompiledDatabase$ZeroCoord <- Output # add new column to register the zero coord occurrences
CompiledDatabase <- CompiledDatabase[CompiledDatabase$ZeroCoord==TRUE, -c("ZeroCoord")] # remove zero coord occurrences
# Identify records with Degree Conversion Error (degree sign treated as decimal delimiter):
CompiledDatabase$IDocc <- 1:nrow(CompiledDatabase)
Output <- CoordinateCleaner::cd_ddmm(CompiledDatabase, lon = "Longitude", lat = "Latitude", ds = "IDocc", value = "flagged", mat_size = 100)
CompiledDatabase$ConvError <- Output  # add new column to register the occurrences with conversion error
CompiledDatabase <- CompiledDatabase[CompiledDatabase$ConvError==TRUE, -c("ConvError")] # remove occurrences with conversion error
# Identify duplicates and remove them (different specimens, same species, same locality):
CompiledDatabase <- CoordinateCleaner::cc_dupl(CompiledDatabase, lon = "Longitude", lat = "Latitude", species = "Species", value = "clean")
# Identify occurrence records over the sea and flag them:
Output <- CoordinateCleaner::cc_sea(CompiledDatabase, lon = "Longitude", lat = "Latitude", value = "flagged", scale = 10)
CompiledDatabase$LandCoord <- Output # add new column to register the dubious occurrences
# Export the map and table of flagged records (manually check/curate if necessary):
flagged_records <- CompiledDatabase[CompiledDatabase$LandCoord=="FALSE",]
data.table::fwrite(x = flagged_records, file = "Datasets/FlaggedRecords.csv")
# Remove occurrence records falling in the ocean:
CompiledDatabase <- CompiledDatabase[CompiledDatabase$LandCoord==TRUE, ] # remove dubious occurrences
CompiledDatabase <- CompiledDatabase[, -c("LandCoord")] # remove the 'LandCoord' column
# Check how many occurrences per species:
N_occ_spp <- CompiledDatabase[,
.N, # count number of rows
by = Species] # provide countings per species
summary(N_occ_spp$N) # check the min and max number of occurrences per species
# Merge the column informing the number of records per species:
CompiledDatabase <- merge(x = CompiledDatabase,
y = N_occ_spp,
by = 'Species',
all.x = TRUE)
# Get the number of species remaining:
N_Spp <- levels(as.factor(CompiledDatabase$Species))
N_Spp
# For didactic purposes, we will use low resolution raster files in the subsequent analysis.
# Perform the spatial thinning of occurrence records (one occurrence per raster pixel or cell):
EnvRaster <- terra::rast("PredictorsLowRes//wc2.1_10m_bio_1.tif") # this was degradaded 2 times (20 minutes)
Occ <- list()
# For didactic purposes, we will use low resolution raster files in the subsequent analysis.
# Perform the spatial thinning of occurrence records (one occurrence per raster pixel or cell):
EnvRaster <- terra::rast("PredictorsLowRes//wc2.1_10m_bio_1.tif") # this was degradaded 2 times (20 minutes)
# Aumentar a resolução do raster
EnvRaster <- disaggregate(EnvRaster, fact = 2)
# Aumentar a resolução do raster
EnvRaster <- terra::aggregate(EnvRaster, fact = 0.5)
# For didactic purposes, we will use low resolution raster files in the subsequent analysis.
# Perform the spatial thinning of occurrence records (one occurrence per raster pixel or cell):
EnvRaster <- terra::rast("PredictorsLowRes//wc2.1_10m_bio_1.tif") # this was degradaded 2 times (20 minutes)
# Aumentar a resolução do raster
EnvRaster <- terra::aggregate(EnvRaster, fact = 0.5)
Occ <- list()
for(i in 1:length(N_Spp)){
try({
# Subset the occurrence records for the species 'i':
Occ_subset <- CompiledDatabase[CompiledDatabase$Species == N_Spp[i], c("Longitude", "Latitude")]
Occ_subset <- as.data.frame(Occ_subset)
names(Occ_subset) <- c("x", "y")
# Compute the spatial thinning of occurrence records:
Output <- flexsdm::occfilt_geo(data = Occ_subset,
x = "x",
y = "y",
method = c("cellsize", factor = "1"),
prj = crs(EnvRaster),
env_layer = EnvRaster)
Occ[[i]] <- data.frame(sp = N_Spp[i],
Output)
# Remove unnecessary objects before next iteration:
rm(Occ_subset, Output)
}, silent=T)
print(i)
}
# Bind all lists in a single data.table:
Occ <- rbindlist(Occ)
View(Occ)
# Load all packages needed for the analysis:
load("RData/PackagesToLoad.RData")
lapply(all_packages, require, character.only = TRUE); rm(all_packages)
# Get a vector of non-installed packages:
new.packages <- all_packages[!(all_packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, repos = "http://cran.us.r-project.org")
# STEP 1.1: PREPARE RASTER FILES TO BE USED AS PREDICTORS AND FOR FUTURE PROJECTIONS (CMIP6 WORLDCLIM v2.1)
########################################################################################################################
# STEP 1.1: PREPARE RASTER FILES TO BE USED AS PREDICTORS AND FOR FUTURE PROJECTIONS (CMIP6 WORLDCLIM v2.1)
rm(list = ls()); gc()
#definir área de estudo como sendo o estado de Minas Gerais a partir do pacote geobr
Background_shp <- geobr::read_state(code_state = "MG", year = 2018)
# Crop the extent of current predictor layers and export files:
path_current_predictors <- "/home/nathan/Documents/modelagem_ecologica/Rasters/Global Predictors (CurrentTime)/"
bioclim_current <- terra::rast(list.files(path = path_current_predictors, pattern = '.tif', full.names = T))
var_names <- names(bioclim_current)
# Extract each raster layer:
for(b in 1:length(var_names)){
# Select one raster layer:
SingleLayer <- terra::subset(x = bioclim_current, subset = b)
# Crop raster bands:
SingleLayer <- terra::crop(x = SingleLayer, y = Background_shp)
# Write the raster:
terra::writeRaster(SingleLayer,
filename = paste0("Predictors/", var_names[b], ".tif"),
overwrite = TRUE)
# Reduce raster resolution to speed up computations:
#SingleLayer <- terra::aggregate(x = SingleLayer, fact = 2, fun = mean, overwrite = TRUE,
#                             filename = paste0("PredictorsLowRes/", var_names[b], ".tif"))
}
# Get a list of all projections:
GlobalProjections <- dir("/home/nathan/Documents/modelagem_ecologica/Rasters/Global Predictors (FutureProjections)/", all.files = F, recursive = T)
# Extract information on SSP, year, GCM, and spatial resolution of each global future projection:
Scenario_Year_GCM <- data.frame(
SSP = paste0("ssp", gsub("_.*", "", gsub(".tif", "", gsub(".*.ssp", "", GlobalProjections)))),
Year = gsub(".*_", "", gsub(".tif", "", gsub(".*.ssp", "", GlobalProjections))),
GCM = gsub(".*._bioc_", "", gsub(".ssp.*", "", GlobalProjections)),
Resolution = gsub("wc2.1_", "", gsub("_bioc_.*", "", gsub(".ssp.*", "", GlobalProjections))),
Filename = GlobalProjections)
# Filter global projections for a specific year or SSP, if necessary:
SelectedProjections <- Scenario_Year_GCM[Scenario_Year_GCM$SSP == "ssp245" & Scenario_Year_GCM$Year == "2081-2100",]
GlobalProjections <- SelectedProjections$Filename
# Unpack multi-band raster into individual files and save them:
for(i in 1:length(GlobalProjections)){
# For each future climate projection, Worldclim V.2.1 provide a single raster file containing 19 bioclimatic bands:
bioclim_future <- terra::rast(paste0("/home/nathan/Documents/modelagem_ecologica/Rasters/Global Predictors (FutureProjections)/", GlobalProjections[i]))
names(bioclim_future)
# Get SSP scenario and year:
folder_name1 <- gsub(".*.ssp", "", GlobalProjections[i])
folder_name1 <- gsub(".tif", "", folder_name1)
# Get the general circulation model (GCM):
folder_name2 <- gsub("_ssp.*", "", GlobalProjections[i])
folder_name2 <- gsub("wc2.1_10m_bioc_", "", folder_name2)
# Define the folder name:
mydir <- paste0("ssp", folder_name1, "_", folder_name2)
# Create the directory using the projection name:
dir.create(paste0("Projections/", mydir), showWarnings = FALSE)
#dir.create(paste0("ProjectionsLowRes/", mydir), showWarnings = FALSE)
# Extract each band and export:
for(b in 1:length(var_names)){
# Load one raster band from the global projection i:
SingleBand <- bioclim_future[[b]]
# Crop the raster band:
SingleBand <- terra::crop(x = SingleBand, y = Background_shp)
# Rename the raster layer:
names(SingleBand) <- paste0("wc2.1_10m_bio_", b) # change the spatial resolution if needed
# Write the raster:
terra::writeRaster(SingleBand, overwrite = TRUE,
filename = paste0("Projections/", mydir, "/", names(SingleBand), ".tif"))
# Reduce raster resolution to speed up computations:
#SingleLayer <- terra::aggregate(x = SingleBand, fact = 2, fun = mean, overwrite = TRUE,
#                             filename = paste0("ProjectionsLowRes/", mydir, "/", names(SingleBand), ".tif"))
}
}
# STEP 2: PERFORM DATA CLEANING PROCEDURES IN THE SPECIES OCCURRENCE DATA
########################################################################################################################
# STEP 2: PERFORM DATA CLEANING PROCEDURES IN THE SPECIES OCCURRENCE DATA
rm(list = ls()); gc()
# Load the database:
CompiledDatabase <- data.table::fread(input = "Datasets/05_cleaned_database.csv", encoding = "UTF-8")
head(CompiledDatabase)
#mudar o nome das colunas 35 e 36 para Longitude e Latitude
names(CompiledDatabase)[35:36] <- c("Latitude", "Longitude")
#mudar o nome da coluna 45 para Species
names(CompiledDatabase)[45] <- "Species"
# Extract the columns of interest:
CompiledDatabase <- CompiledDatabase[, c("Species", "Longitude", "Latitude", "ID")] # works for data.frame and data.table objects
# Remove rows without geographical coordinates:
CompiledDatabase <- CompiledDatabase[!is.na(CompiledDatabase$Latitude),]
CompiledDatabase <- CompiledDatabase[!is.na(CompiledDatabase$Longitude),]
# Check how many species:
nlevels(as.factor(CompiledDatabase$Species))
#carregar a área de estudo como sendo o estado de Minas Gerais a partir do pacote geobr
study_area <- geobr::read_state(code_state = "MG", year = 2018)
# Let's check the structure of attributes within the loaded shapefile:
head(study_area)
# Create a object containing the coordinate reference system (crs) WGS84:
wgs84 <- "+proj=longlat +datum=WGS84 +no_defs"
pts <- sp::SpatialPoints(CompiledDatabase[, c("Longitude","Latitude")], proj4string = sp::CRS(wgs84))
study_area <- sf::st_transform(study_area, st_crs(pts))
# Check which occurrence records are within the desired ecoregion:
study_area <- sf::as_Spatial(study_area) # convert from sf to sp object
WithinStudyArea <- sp::over(x = pts, y = study_area)[, 1]
summary(WithinStudyArea)
rm(pts) # remove unnecessary objects
# Combine the 'WithinStudyArea' object with the CompiledDatabase:
CompiledDatabase <- cbind(CompiledDatabase, WithinStudyArea)
rm(WithinStudyArea)
# Identify which species have records within the study area:
CompiledDatabase$Species <- as.character(CompiledDatabase$Species) # convert to character to avoid matching problems
Target_spp <- unique(CompiledDatabase[!is.na(CompiledDatabase$WithinStudyArea),]$Species) # vector of species names
# Now, filter the CompiledDatabase to contain only the target species:
CompiledDatabase <- CompiledDatabase[Species %in% Target_spp,]
CompiledDatabase <- CompiledDatabase[ , c("Species","Longitude", "Latitude", "ID")] # keep only essential information
# Identify invalid Coordinates:
Output <- CoordinateCleaner::cc_val(CompiledDatabase, lon = "Longitude", lat = "Latitude", value = "flagged", verbose = T)
CompiledDatabase$InvalidOcc <- Output # add new column to register the dubious occurrences
CompiledDatabase <- CompiledDatabase[CompiledDatabase$InvalidOcc == TRUE, -c("InvalidOcc")] # remove dubious occurrences
# Identify Zero Coordinates (longitude = 0, latitude = 0):
Output <- CoordinateCleaner::cc_zero(CompiledDatabase, lon = "Longitude", lat = "Latitude", value = "flagged", verbose = T)
CompiledDatabase$ZeroCoord <- Output # add new column to register the zero coord occurrences
CompiledDatabase <- CompiledDatabase[CompiledDatabase$ZeroCoord==TRUE, -c("ZeroCoord")] # remove zero coord occurrences
# Identify records with Degree Conversion Error (degree sign treated as decimal delimiter):
CompiledDatabase$IDocc <- 1:nrow(CompiledDatabase)
Output <- CoordinateCleaner::cd_ddmm(CompiledDatabase, lon = "Longitude", lat = "Latitude", ds = "IDocc", value = "flagged", mat_size = 100)
CompiledDatabase$ConvError <- Output  # add new column to register the occurrences with conversion error
CompiledDatabase <- CompiledDatabase[CompiledDatabase$ConvError==TRUE, -c("ConvError")] # remove occurrences with conversion error
# Identify duplicates and remove them (different specimens, same species, same locality):
CompiledDatabase <- CoordinateCleaner::cc_dupl(CompiledDatabase, lon = "Longitude", lat = "Latitude", species = "Species", value = "clean")
# Corrigir latitude e longitude transpostas
check_pf$database_id <- 1:nrow(check_pf)
library(devtools)
devtools::install_github('gilles-guillot/Geneland', build_vignettes = TRUE)
devtools::install_github('gilles-guillot/Geneland', build_vignettes = TRUE)
library(tcltk)
library(tcltk, lib.loc = "/usr/local/lib/R/library")
library(tcltk)
detach("package:tcltk", unload = TRUE)
library(tcltk, lib.loc = "/usr/local/lib/R/library")
system("R CMD config --libs")
library(tcltk, lib.loc = "/usr/local/lib/R/library")
sudo apt-get update
library(tcltk, lib.loc = "/usr/local/lib/R/library")
capabilities("tcltk")
capabilities("tcltk")
clear
library(Geneland)
# Carrega a biblioteca Geneland
library(Geneland)
# Define o diretório principal onde serão criadas pastas e salvos os resultados
maindir <- "/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/Geneland/Scripts/Geneland_analysis/"
setwd(maindir)  # Define o diretório de trabalho
# Carrega os arquivos de entrada
# Carrega a matriz genotípica (nuclear)
geno.temp <- read.table("Proceratophrys_boiei_matrix_sorted_nuclear_no_individual_column_NEW.txt", na.string="NA")
geno <- as.matrix(geno.temp)  # Converte para matriz
# Carrega as coordenadas geográficas dos indivíduos
coord.temp <- read.table("/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/Geneland/Scripts/Proceratophrys_boiei_haplotype_matrix_sorted_nuclear_with_individuals_NEW_geo.txt", na.string='NA')
coord <- as.matrix(coord.temp)
# Carrega os dados mitocondriais
mt.temp <- read.table('/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/Geneland/Scripts/Geneland_analysis/filtered_mitochondrial_data_no_header.txt', na.string='NA')
mt <- as.matrix(mt.temp)
maindir <- "/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/Geneland/Scripts/Geneland_analysis/resultsnm" #### dir
nrun <- 8        ### number of runs
burnin <- 200     ### burnin
chain <- 500000  ### number of steps in chain
freq <-      500 ### sampling frequency
pop <- 5          ### max number of pops
for(irun in 1:nrun)
{
## define path to MCMC directory
path.mcmc <- paste(maindir,'/',irun,"/",sep="")
### create directory
if (file.exists(path.mcmc)){
setwd(file.path(path.mcmc))
} else {
dir.create(file.path(path.mcmc))
setwd(file.path(path.mcmc))
}
system(paste(path.mcmc))
MCMC(coordinates=coord,
#geno.dip.codom=geno,
geno.hap=mt,
rate.max = 42,
nb.nuclei.max = 126,
delta.coord = 0.01,
varnpop=TRUE,
npopmax=pop,
spatial=TRUE,
freq.model="Uncorrelated",
nit=chain,
thinning=freq,
path.mcmc=path.mcmc)
PostProcessChain(coordinates=coord,
path.mcmc=path.mcmc,
nxdom=200,
nydom=200,
burnin=burnin)
}
for(irun in 1:nrun)
{
## define path to MCMC directory
path.mcmc <- paste(maindir,'/',irun,"/",sep="")
### create directory
if (file.exists(path.mcmc)){
setwd(file.path(path.mcmc))
} else {
dir.create(file.path(path.mcmc))
setwd(file.path(path.mcmc))
}
system(paste(path.mcmc))
MCMC(coordinates=coord,
#geno.dip.codom=geno,
geno.hap=mt,
rate.max = 42,
nb.nuclei.max = 126,
delta.coord = 0.01,
varnpop=TRUE,
npopmax=pop,
spatial=TRUE,
freq.model="Uncorrelated",
nit=chain,
thinning=freq,
null.alleles=FALSE,
path.mcmc=path.mcmc)
PostProcessChain(coordinates=coord,
path.mcmc=path.mcmc,
nxdom=200,
nydom=200,
burnin=burnin)
}
#### log posterior density calculation ###
lpd <- rep(NA,nrun)
for(irun in 1:nrun)
{
## define path to MCMC directory
path.mcmc <- paste(maindir,'/',irun,"/",sep="")
### create directory
if (file.exists(path.mcmc)){
setwd(file.path(path.mcmc))
} else {
dir.create(file.path(path.mcmc))
setwd(file.path(path.mcmc))
}
system(paste(path.mcmc))
MCMC(coordinates=coord,
#geno.dip.codom=geno,
geno.hap=mt,
rate.max = 42,
nb.nuclei.max = 126,
delta.coord = 0.01,
varnpop=TRUE,
npopmax=pop,
spatial=TRUE,
freq.model="Uncorrelated",
nit=chain,
thinning=freq,
null.loci=FALSE,
path.mcmc=path.mcmc)
PostProcessChain(coordinates=coord,
path.mcmc=path.mcmc,
nxdom=200,
nydom=200,
burnin=burnin)
}
?MCMC
for(irun in 1:nrun)
{
## define path to MCMC directory
path.mcmc <- paste(maindir,'/',irun,"/",sep="")
### create directory
if (file.exists(path.mcmc)){
setwd(file.path(path.mcmc))
} else {
dir.create(file.path(path.mcmc))
setwd(file.path(path.mcmc))
}
system(paste(path.mcmc))
MCMC(coordinates=coord,
#geno.dip.codom=geno,
geno.hap=mt,
rate.max = 42,
nb.nuclei.max = 126,
delta.coord = 0.01,
varnpop=TRUE,
npopmax=pop,
spatial=TRUE,
freq.model="Uncorrelated",
nit=chain,
thinning=freq,
filter.null.alleles = FALSE,
path.mcmc=path.mcmc)
PostProcessChain(coordinates=coord,
path.mcmc=path.mcmc,
nxdom=200,
nydom=200,
burnin=burnin)
}
#### log posterior density calculation ###
lpd <- rep(NA,nrun)
for (irun in 1:nrun) {
path.mcmc <- paste(maindir, "/", irun, "/", sep="") # Ajuste para "10_Run"
path.lpd <- paste(path.mcmc, "log.posterior.density.txt", sep="")
if (file.exists(path.lpd)) { # Verifica se o arquivo existe
lpd[irun] <- mean(scan(path.lpd)[-(1:burnin)])
} else {
warning(paste("Arquivo não encontrado:", path.lpd))
lpd[irun] <- NA
}
}
order(lpd,decreasing=TRUE)
lpd
for(irun in 1:nrun)
{
path.mcmc <- paste(maindir,"/",irun,"/",sep="")
setwd(file.path(path.mcmc))
Plotnpop(path.mcmc=path.mcmc, burnin=200)
}
for(irun in 1:nrun)
{
path.mcmc <- paste(maindir,"/",irun,"/",sep="")
setwd(file.path(path.mcmc))
PosteriorMode(coordinates=coord, path.mcmc="./", file="map.pdf")
}
# Cria um mapa de probabilidade de pertencimento aos clusters
PlotTessellation(coord, path.mcmc='./', printit=TRUE, path='./')
maindir <- "/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/Geneland/Scripts/Geneland_analysis/resultsnm" #### dir
maindir <- "/home/nathan/Documents/Doutorado_diversidade_genética/Phylogeogra/Geneland/Scripts/Geneland_analysis/resultsnm" #### dir
nrun <- 5        ### number of runs
burnin <- 1000     ### burnin
chain <- 1000000  ### number of steps in chain
freq <-      100 ### sampling frequency
pop <- 10          ### max number of pops
for(irun in 1:nrun)
{
## define path to MCMC directory
path.mcmc <- paste(maindir,'/',irun,"/",sep="")
### create directory
if (file.exists(path.mcmc)){
setwd(file.path(path.mcmc))
} else {
dir.create(file.path(path.mcmc))
setwd(file.path(path.mcmc))
}
system(paste(path.mcmc))
MCMC(coordinates=coord,
#geno.dip.codom=geno,
geno.hap=mt,
rate.max = 42,
nb.nuclei.max = 126,
delta.coord = 0.01,
varnpop=TRUE,
npopmax=pop,
spatial=TRUE,
freq.model="Uncorrelated",
nit=chain,
thinning=freq,
filter.null.alleles = FALSE,
path.mcmc=path.mcmc)
PostProcessChain(coordinates=coord,
path.mcmc=path.mcmc,
nxdom=200,
nydom=200,
burnin=burnin)
}
#### log posterior density calculation ###
lpd <- rep(NA,nrun)
for (irun in 1:nrun) {
path.mcmc <- paste(maindir, "/", irun, "/", sep="") # Ajuste para "10_Run"
path.lpd <- paste(path.mcmc, "log.posterior.density.txt", sep="")
if (file.exists(path.lpd)) { # Verifica se o arquivo existe
lpd[irun] <- mean(scan(path.lpd)[-(1:burnin)])
} else {
warning(paste("Arquivo não encontrado:", path.lpd))
lpd[irun] <- NA
}
}
order(lpd,decreasing=TRUE)
lpd
for(irun in 1:nrun)
{
path.mcmc <- paste(maindir,"/",irun,"/",sep="")
setwd(file.path(path.mcmc))
Plotnpop(path.mcmc=path.mcmc, burnin=200)
}
for(irun in 1:nrun)
{
path.mcmc <- paste(maindir,"/",irun,"/",sep="")
setwd(file.path(path.mcmc))
PosteriorMode(coordinates=coord, path.mcmc="./", file="map.pdf")
}
# Cria um mapa de probabilidade de pertencimento aos clusters
PlotTessellation(coord, path.mcmc='./', printit=TRUE, path='./')
